{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "- CBoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6f5e89fd90>"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "\n",
    "\n",
    "vocab = list(set(sentence))\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty', 'shall', 'besiege'], 'winters'), (['forty', 'winters', 'besiege', 'thy'], 'shall'), (['winters', 'shall', 'thy', 'brow,'], 'besiege')]\n",
      "\n",
      "\n",
      "{'praise.': 0, 'a': 1, 'besiege': 2, 'it': 3, 'deep': 4, 'be': 5, 'lies,': 6, 'field,': 7, 'sunken': 8, 'made': 9, 'all': 10, 'proud': 11, 'in': 12, 'To': 13, \"beauty's\": 14, 'Proving': 15, 'treasure': 16, 'my': 17, 'see': 18, 'livery': 19, 'eyes,': 20, \"'This\": 21, 'sum': 22, 'forty': 23, 'use,': 24, 'own': 25, 'praise': 26, 'old,': 27, 'brow,': 28, 'so': 29, 'say,': 30, 'blood': 31, \"excuse,'\": 32, 'gazed': 33, 'When': 34, 'worth': 35, 'thou': 36, 'old': 37, 'make': 38, 'were': 39, 'his': 40, 'more': 41, 'small': 42, 'count,': 43, 'thy': 44, 'mine': 45, 'succession': 46, 'asked,': 47, 'beauty': 48, 'Then': 49, 'fair': 50, \"feel'st\": 51, 'by': 52, 'within': 53, 'held:': 54, 'couldst': 55, 'now,': 56, 'an': 57, 'shame,': 58, 'And': 59, 'the': 60, 'when': 61, 'to': 62, 'child': 63, 'lusty': 64, 'much': 65, 'Will': 66, 'shall': 67, 'Thy': 68, 'answer': 69, 'This': 70, 'where': 71, 'all-eating': 72, 'art': 73, 'If': 74, 'winters': 75, 'thine': 76, 'days;': 77, 'cold.': 78, 'Shall': 79, \"totter'd\": 80, 'Were': 81, 'of': 82, 'on': 83, 'trenches': 84, 'thine!': 85, 'dig': 86, 'and': 87, 'How': 88, \"deserv'd\": 89, 'warm': 90, 'Where': 91, 'being': 92, 'weed': 93, 'new': 94, \"youth's\": 95, 'thriftless': 96}\n"
     ]
    }
   ],
   "source": [
    "data = \\\n",
    "[([sentence[idx-2],sentence[idx-1],sentence[idx+1],sentence[idx+2]],sentence[idx])\\\n",
    " for idx in range(2,len(sentence)-2)]\n",
    "print(data[:3],end='\\n\\n\\n')\n",
    "\n",
    "word_to_ix = {val : idx for idx,val in enumerate(vocab)}\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module) : \n",
    "    \n",
    "    def __init__(self,vocab_size, embedding_dim):  \n",
    "        \n",
    "        super(CBOW,self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_size\n",
    "        self.embeddings = nn.Embedding(self.vocab_size,self.embedding_dim)\n",
    "        self.linear1 = nn.Linear(self.embedding_dim,128)\n",
    "        self.linear2 = nn.Linear(128,self.vocab_size)\n",
    "    \n",
    "    def forward(self,inputs) : \n",
    "        self.embeds = self.embeddings(inputs).sum(dim=0).unsqueeze(0)\n",
    "        out = F.relu(self.linear1(self.embeds))\n",
    "        out = self.linear2(out)\n",
    "            \n",
    "        log_probs = F.log_softmax(out,dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "    def get_word_vector(self,target,word_to_ix) : \n",
    "        word2id = torch.LongTensor([word_to_ix[target]])\n",
    "        return self.embeddings(word2id).view(1, -1)\n",
    "        \n",
    "    \n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    word2id = torch.tensor(idxs, dtype=torch.long)\n",
    "    return word2id\n",
    "\n",
    "def make_target_vector(target, word_to_ix):\n",
    "    idxs = [word_to_ix[target]]\n",
    "    return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "EPOCH = 20\n",
    "VERVOSE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cbow(vocab_size , word_to_ix) :\n",
    "    loss_function = nn.NLLLoss()\n",
    "    model = CBOW(vocab_size, EMBEDDING_DIM)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        total_loss = 0\n",
    "        for context, target in data:\n",
    "\n",
    "            # 아래의 프로세스는 word : ID 로 converting 시켜주는 process 이다.\n",
    "            context_idxs = make_context_vector(context,word_to_ix)\n",
    "            target_idxs = torch.LongTensor([word_to_ix[target]])\n",
    "\n",
    "            # pytorch는 gradient 를 누진적으로 계산하기 때문에, 0으로 만들어주어야한다.\n",
    "            model.zero_grad()\n",
    "\n",
    "            # input 값을 넣으면 log_probs 라는 output 값이 나온다.\n",
    "            log_probs = model(context_idxs)\n",
    "\n",
    "            # 이 값을 위에서 정의한 손실 함수를 기반으로 loss 를 계산한다.\n",
    "            loss = loss_function(log_probs, target_idxs)\n",
    "\n",
    "            # 위에서 나온 loss를 기반으로 back propagation 을 돌린다.\n",
    "            # 또한, optimizer 를 update 하면서 parameter 또한 update 한다.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss\n",
    "\n",
    "            if epoch % VERVOSE == 0 : \n",
    "                loss_avg = float(total_loss / len(data))\n",
    "                print(\"{}/{} loss {:.2f}\".format(epoch, EPOCH, loss_avg))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20 loss 0.05\n",
      "0/20 loss 0.09\n",
      "0/20 loss 0.14\n",
      "0/20 loss 0.17\n",
      "0/20 loss 0.21\n",
      "0/20 loss 0.25\n",
      "0/20 loss 0.29\n",
      "0/20 loss 0.34\n",
      "0/20 loss 0.39\n",
      "0/20 loss 0.43\n",
      "0/20 loss 0.47\n",
      "0/20 loss 0.51\n",
      "0/20 loss 0.55\n",
      "0/20 loss 0.59\n",
      "0/20 loss 0.63\n",
      "0/20 loss 0.67\n",
      "0/20 loss 0.71\n",
      "0/20 loss 0.75\n",
      "0/20 loss 0.79\n",
      "0/20 loss 0.83\n",
      "0/20 loss 0.87\n",
      "0/20 loss 0.91\n",
      "0/20 loss 0.95\n",
      "0/20 loss 0.99\n",
      "0/20 loss 1.04\n",
      "0/20 loss 1.09\n",
      "0/20 loss 1.13\n",
      "0/20 loss 1.18\n",
      "0/20 loss 1.22\n",
      "0/20 loss 1.27\n",
      "0/20 loss 1.31\n",
      "0/20 loss 1.35\n",
      "0/20 loss 1.39\n",
      "0/20 loss 1.43\n",
      "0/20 loss 1.48\n",
      "0/20 loss 1.52\n",
      "0/20 loss 1.56\n",
      "0/20 loss 1.60\n",
      "0/20 loss 1.64\n",
      "0/20 loss 1.69\n",
      "0/20 loss 1.73\n",
      "0/20 loss 1.78\n",
      "0/20 loss 1.80\n",
      "0/20 loss 1.84\n",
      "0/20 loss 1.88\n",
      "0/20 loss 1.93\n",
      "0/20 loss 1.96\n",
      "0/20 loss 2.01\n",
      "0/20 loss 2.05\n",
      "0/20 loss 2.09\n",
      "0/20 loss 2.13\n",
      "0/20 loss 2.17\n",
      "0/20 loss 2.21\n",
      "0/20 loss 2.26\n",
      "0/20 loss 2.30\n",
      "0/20 loss 2.34\n",
      "0/20 loss 2.39\n",
      "0/20 loss 2.43\n",
      "0/20 loss 2.47\n",
      "0/20 loss 2.51\n",
      "0/20 loss 2.56\n",
      "0/20 loss 2.60\n",
      "0/20 loss 2.64\n",
      "0/20 loss 2.68\n",
      "0/20 loss 2.72\n",
      "0/20 loss 2.77\n",
      "0/20 loss 2.80\n",
      "0/20 loss 2.84\n",
      "0/20 loss 2.89\n",
      "0/20 loss 2.93\n",
      "0/20 loss 2.97\n",
      "0/20 loss 3.02\n",
      "0/20 loss 3.06\n",
      "0/20 loss 3.10\n",
      "0/20 loss 3.15\n",
      "0/20 loss 3.20\n",
      "0/20 loss 3.24\n",
      "0/20 loss 3.29\n",
      "0/20 loss 3.32\n",
      "0/20 loss 3.36\n",
      "0/20 loss 3.40\n",
      "0/20 loss 3.44\n",
      "0/20 loss 3.48\n",
      "0/20 loss 3.52\n",
      "0/20 loss 3.56\n",
      "0/20 loss 3.61\n",
      "0/20 loss 3.65\n",
      "0/20 loss 3.70\n",
      "0/20 loss 3.74\n",
      "0/20 loss 3.78\n",
      "0/20 loss 3.83\n",
      "0/20 loss 3.87\n",
      "0/20 loss 3.91\n",
      "0/20 loss 3.96\n",
      "0/20 loss 4.00\n",
      "0/20 loss 4.04\n",
      "0/20 loss 4.08\n",
      "0/20 loss 4.12\n",
      "0/20 loss 4.17\n",
      "0/20 loss 4.23\n",
      "0/20 loss 4.27\n",
      "0/20 loss 4.31\n",
      "0/20 loss 4.35\n",
      "0/20 loss 4.39\n",
      "0/20 loss 4.43\n",
      "0/20 loss 4.46\n",
      "0/20 loss 4.50\n",
      "0/20 loss 4.55\n",
      "0/20 loss 4.59\n",
      "0/20 loss 4.64\n",
      "0/20 loss 4.68\n",
      "5/20 loss 0.05\n",
      "5/20 loss 0.09\n",
      "5/20 loss 0.13\n",
      "5/20 loss 0.16\n",
      "5/20 loss 0.19\n",
      "5/20 loss 0.23\n",
      "5/20 loss 0.27\n",
      "5/20 loss 0.30\n",
      "5/20 loss 0.35\n",
      "5/20 loss 0.39\n",
      "5/20 loss 0.41\n",
      "5/20 loss 0.46\n",
      "5/20 loss 0.49\n",
      "5/20 loss 0.53\n",
      "5/20 loss 0.57\n",
      "5/20 loss 0.61\n",
      "5/20 loss 0.65\n",
      "5/20 loss 0.69\n",
      "5/20 loss 0.73\n",
      "5/20 loss 0.76\n",
      "5/20 loss 0.80\n",
      "5/20 loss 0.84\n",
      "5/20 loss 0.87\n",
      "5/20 loss 0.92\n",
      "5/20 loss 0.96\n",
      "5/20 loss 1.01\n",
      "5/20 loss 1.05\n",
      "5/20 loss 1.09\n",
      "5/20 loss 1.13\n",
      "5/20 loss 1.18\n",
      "5/20 loss 1.21\n",
      "5/20 loss 1.26\n",
      "5/20 loss 1.29\n",
      "5/20 loss 1.33\n",
      "5/20 loss 1.37\n",
      "5/20 loss 1.41\n",
      "5/20 loss 1.45\n",
      "5/20 loss 1.49\n",
      "5/20 loss 1.53\n",
      "5/20 loss 1.57\n",
      "5/20 loss 1.61\n",
      "5/20 loss 1.66\n",
      "5/20 loss 1.68\n",
      "5/20 loss 1.70\n",
      "5/20 loss 1.74\n",
      "5/20 loss 1.78\n",
      "5/20 loss 1.82\n",
      "5/20 loss 1.86\n",
      "5/20 loss 1.90\n",
      "5/20 loss 1.94\n",
      "5/20 loss 1.98\n",
      "5/20 loss 2.02\n",
      "5/20 loss 2.05\n",
      "5/20 loss 2.10\n",
      "5/20 loss 2.14\n",
      "5/20 loss 2.18\n",
      "5/20 loss 2.22\n",
      "5/20 loss 2.26\n",
      "5/20 loss 2.30\n",
      "5/20 loss 2.34\n",
      "5/20 loss 2.38\n",
      "5/20 loss 2.42\n",
      "5/20 loss 2.46\n",
      "5/20 loss 2.50\n",
      "5/20 loss 2.54\n",
      "5/20 loss 2.58\n",
      "5/20 loss 2.61\n",
      "5/20 loss 2.64\n",
      "5/20 loss 2.69\n",
      "5/20 loss 2.73\n",
      "5/20 loss 2.77\n",
      "5/20 loss 2.81\n",
      "5/20 loss 2.85\n",
      "5/20 loss 2.89\n",
      "5/20 loss 2.93\n",
      "5/20 loss 2.97\n",
      "5/20 loss 3.00\n",
      "5/20 loss 3.05\n",
      "5/20 loss 3.08\n",
      "5/20 loss 3.11\n",
      "5/20 loss 3.15\n",
      "5/20 loss 3.19\n",
      "5/20 loss 3.23\n",
      "5/20 loss 3.26\n",
      "5/20 loss 3.30\n",
      "5/20 loss 3.34\n",
      "5/20 loss 3.39\n",
      "5/20 loss 3.43\n",
      "5/20 loss 3.47\n",
      "5/20 loss 3.50\n",
      "5/20 loss 3.55\n",
      "5/20 loss 3.58\n",
      "5/20 loss 3.63\n",
      "5/20 loss 3.67\n",
      "5/20 loss 3.71\n",
      "5/20 loss 3.75\n",
      "5/20 loss 3.78\n",
      "5/20 loss 3.82\n",
      "5/20 loss 3.86\n",
      "5/20 loss 3.91\n",
      "5/20 loss 3.95\n",
      "5/20 loss 3.99\n",
      "5/20 loss 4.02\n",
      "5/20 loss 4.06\n",
      "5/20 loss 4.09\n",
      "5/20 loss 4.12\n",
      "5/20 loss 4.16\n",
      "5/20 loss 4.20\n",
      "5/20 loss 4.24\n",
      "5/20 loss 4.28\n",
      "5/20 loss 4.32\n",
      "10/20 loss 0.05\n",
      "10/20 loss 0.08\n",
      "10/20 loss 0.12\n",
      "10/20 loss 0.14\n",
      "10/20 loss 0.17\n",
      "10/20 loss 0.21\n",
      "10/20 loss 0.25\n",
      "10/20 loss 0.28\n",
      "10/20 loss 0.32\n",
      "10/20 loss 0.35\n",
      "10/20 loss 0.37\n",
      "10/20 loss 0.42\n",
      "10/20 loss 0.45\n",
      "10/20 loss 0.49\n",
      "10/20 loss 0.53\n",
      "10/20 loss 0.56\n",
      "10/20 loss 0.60\n",
      "10/20 loss 0.65\n",
      "10/20 loss 0.68\n",
      "10/20 loss 0.72\n",
      "10/20 loss 0.76\n",
      "10/20 loss 0.79\n",
      "10/20 loss 0.82\n",
      "10/20 loss 0.86\n",
      "10/20 loss 0.90\n",
      "10/20 loss 0.95\n",
      "10/20 loss 0.99\n",
      "10/20 loss 1.02\n",
      "10/20 loss 1.06\n",
      "10/20 loss 1.10\n",
      "10/20 loss 1.14\n",
      "10/20 loss 1.18\n",
      "10/20 loss 1.21\n",
      "10/20 loss 1.25\n",
      "10/20 loss 1.29\n",
      "10/20 loss 1.33\n",
      "10/20 loss 1.37\n",
      "10/20 loss 1.41\n",
      "10/20 loss 1.44\n",
      "10/20 loss 1.48\n",
      "10/20 loss 1.52\n",
      "10/20 loss 1.56\n",
      "10/20 loss 1.58\n",
      "10/20 loss 1.60\n",
      "10/20 loss 1.64\n",
      "10/20 loss 1.67\n",
      "10/20 loss 1.71\n",
      "10/20 loss 1.75\n",
      "10/20 loss 1.79\n",
      "10/20 loss 1.83\n",
      "10/20 loss 1.86\n",
      "10/20 loss 1.90\n",
      "10/20 loss 1.93\n",
      "10/20 loss 1.98\n",
      "10/20 loss 2.01\n",
      "10/20 loss 2.05\n",
      "10/20 loss 2.10\n",
      "10/20 loss 2.13\n",
      "10/20 loss 2.17\n",
      "10/20 loss 2.21\n",
      "10/20 loss 2.25\n",
      "10/20 loss 2.28\n",
      "10/20 loss 2.32\n",
      "10/20 loss 2.35\n",
      "10/20 loss 2.39\n",
      "10/20 loss 2.43\n",
      "10/20 loss 2.46\n",
      "10/20 loss 2.49\n",
      "10/20 loss 2.54\n",
      "10/20 loss 2.57\n",
      "10/20 loss 2.61\n",
      "10/20 loss 2.65\n",
      "10/20 loss 2.68\n",
      "10/20 loss 2.73\n",
      "10/20 loss 2.77\n",
      "10/20 loss 2.79\n",
      "10/20 loss 2.82\n",
      "10/20 loss 2.86\n",
      "10/20 loss 2.90\n",
      "10/20 loss 2.93\n",
      "10/20 loss 2.97\n",
      "10/20 loss 3.00\n",
      "10/20 loss 3.03\n",
      "10/20 loss 3.06\n",
      "10/20 loss 3.10\n",
      "10/20 loss 3.14\n",
      "10/20 loss 3.18\n",
      "10/20 loss 3.22\n",
      "10/20 loss 3.25\n",
      "10/20 loss 3.29\n",
      "10/20 loss 3.33\n",
      "10/20 loss 3.36\n",
      "10/20 loss 3.40\n",
      "10/20 loss 3.45\n",
      "10/20 loss 3.49\n",
      "10/20 loss 3.52\n",
      "10/20 loss 3.55\n",
      "10/20 loss 3.59\n",
      "10/20 loss 3.62\n",
      "10/20 loss 3.66\n",
      "10/20 loss 3.70\n",
      "10/20 loss 3.73\n",
      "10/20 loss 3.77\n",
      "10/20 loss 3.80\n",
      "10/20 loss 3.83\n",
      "10/20 loss 3.86\n",
      "10/20 loss 3.89\n",
      "10/20 loss 3.93\n",
      "10/20 loss 3.97\n",
      "10/20 loss 4.00\n",
      "10/20 loss 4.04\n",
      "15/20 loss 0.04\n",
      "15/20 loss 0.08\n",
      "15/20 loss 0.12\n",
      "15/20 loss 0.13\n",
      "15/20 loss 0.16\n",
      "15/20 loss 0.19\n",
      "15/20 loss 0.23\n",
      "15/20 loss 0.25\n",
      "15/20 loss 0.29\n",
      "15/20 loss 0.32\n",
      "15/20 loss 0.34\n",
      "15/20 loss 0.38\n",
      "15/20 loss 0.41\n",
      "15/20 loss 0.45\n",
      "15/20 loss 0.49\n",
      "15/20 loss 0.53\n",
      "15/20 loss 0.57\n",
      "15/20 loss 0.61\n",
      "15/20 loss 0.65\n",
      "15/20 loss 0.68\n",
      "15/20 loss 0.72\n",
      "15/20 loss 0.75\n",
      "15/20 loss 0.77\n",
      "15/20 loss 0.82\n",
      "15/20 loss 0.86\n",
      "15/20 loss 0.90\n",
      "15/20 loss 0.93\n",
      "15/20 loss 0.97\n",
      "15/20 loss 1.01\n",
      "15/20 loss 1.04\n",
      "15/20 loss 1.08\n",
      "15/20 loss 1.12\n",
      "15/20 loss 1.15\n",
      "15/20 loss 1.18\n",
      "15/20 loss 1.22\n",
      "15/20 loss 1.26\n",
      "15/20 loss 1.30\n",
      "15/20 loss 1.33\n",
      "15/20 loss 1.37\n",
      "15/20 loss 1.41\n",
      "15/20 loss 1.44\n",
      "15/20 loss 1.48\n",
      "15/20 loss 1.50\n",
      "15/20 loss 1.52\n",
      "15/20 loss 1.55\n",
      "15/20 loss 1.58\n",
      "15/20 loss 1.61\n",
      "15/20 loss 1.66\n",
      "15/20 loss 1.70\n",
      "15/20 loss 1.73\n",
      "15/20 loss 1.77\n",
      "15/20 loss 1.80\n",
      "15/20 loss 1.83\n",
      "15/20 loss 1.88\n",
      "15/20 loss 1.91\n",
      "15/20 loss 1.95\n",
      "15/20 loss 1.99\n",
      "15/20 loss 2.03\n",
      "15/20 loss 2.06\n",
      "15/20 loss 2.10\n",
      "15/20 loss 2.13\n",
      "15/20 loss 2.17\n",
      "15/20 loss 2.21\n",
      "15/20 loss 2.23\n",
      "15/20 loss 2.27\n",
      "15/20 loss 2.31\n",
      "15/20 loss 2.33\n",
      "15/20 loss 2.37\n",
      "15/20 loss 2.41\n",
      "15/20 loss 2.44\n",
      "15/20 loss 2.48\n",
      "15/20 loss 2.51\n",
      "15/20 loss 2.55\n",
      "15/20 loss 2.59\n",
      "15/20 loss 2.62\n",
      "15/20 loss 2.64\n",
      "15/20 loss 2.67\n",
      "15/20 loss 2.71\n",
      "15/20 loss 2.74\n",
      "15/20 loss 2.77\n",
      "15/20 loss 2.81\n",
      "15/20 loss 2.84\n",
      "15/20 loss 2.86\n",
      "15/20 loss 2.90\n",
      "15/20 loss 2.93\n",
      "15/20 loss 2.97\n",
      "15/20 loss 3.01\n",
      "15/20 loss 3.04\n",
      "15/20 loss 3.07\n",
      "15/20 loss 3.11\n",
      "15/20 loss 3.15\n",
      "15/20 loss 3.17\n",
      "15/20 loss 3.22\n",
      "15/20 loss 3.26\n",
      "15/20 loss 3.29\n",
      "15/20 loss 3.33\n",
      "15/20 loss 3.35\n",
      "15/20 loss 3.39\n",
      "15/20 loss 3.42\n",
      "15/20 loss 3.45\n",
      "15/20 loss 3.49\n",
      "15/20 loss 3.52\n",
      "15/20 loss 3.55\n",
      "15/20 loss 3.58\n",
      "15/20 loss 3.61\n",
      "15/20 loss 3.64\n",
      "15/20 loss 3.67\n",
      "15/20 loss 3.70\n",
      "15/20 loss 3.74\n",
      "15/20 loss 3.77\n",
      "15/20 loss 3.80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (embeddings): Embedding(97, 5)\n",
       "  (linear1): Linear(in_features=5, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=97, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cbow(vocab_size , word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(vocab_size,EMBEDDING_DIM)\n",
    "\n",
    "def test_cbow(model , vocab , word_to_ix) :\n",
    "    word_1 = vocab[10]\n",
    "    word_2 = vocab[5]\n",
    "    \n",
    "    word_1_vec = model.get_word_vector(word_1,word_to_ix)\n",
    "    word_2_vec = model.get_word_vector(word_2,word_to_ix)\n",
    "    \n",
    "    cosine_similarity = (torch.mm(word_1_vec, word_2_vec.transpose(0,1))) / (torch.norm(word_1_vec) * torch.norm(word_2_vec))\n",
    "    similarity = cosine_similarity.data.numpy()[0][0]\n",
    "    print('word1 : ',word_1)\n",
    "    print('word2 : ',word_2)\n",
    "    print('similarity : ',similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word1 :  all\n",
      "word2 :  be\n",
      "similarity :  -0.68877524\n"
     ]
    }
   ],
   "source": [
    "test_cbow(model,vocab,word_to_ix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
