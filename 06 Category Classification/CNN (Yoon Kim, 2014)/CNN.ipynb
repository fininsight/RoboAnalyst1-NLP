{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data는 e9t(Lucy Park)님께서 github에 공유해주신 네이버 영화평점 데이터를 사용하였습니다.\n",
    "# https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# data를 읽어옴\n",
    "def read_txt(path_to_file):\n",
    "    txt_ls = []\n",
    "    label_ls = []\n",
    "\n",
    "    with open(path_to_file) as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            id_num, txt, label = line.split('\\t')\n",
    "            txt_ls.append(txt)\n",
    "            label_ls.append(int(label.replace('\\n','')))\n",
    "    return txt_ls, label_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "x_train, y_train = read_txt('../ratings_train.txt')\n",
    "x_test, y_test = read_txt('../ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 더빙.. 진짜 짜증나네요 목소리'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비어있는 리뷰 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_review(X, Y):\n",
    "    empty_idx_ls = []\n",
    "    \n",
    "    for idx, review in enumerate(X):\n",
    "        if len(review) == 0:\n",
    "            empty_idx_ls.append(idx)\n",
    "    \n",
    "    # idx 값이 큰 것부터 제거 (앞으로 밀리는 것을 방지)\n",
    "    empty_idx_ls = sorted(empty_idx_ls, reverse = True)\n",
    "    \n",
    "    for empty_idx in empty_idx_ls:\n",
    "        del X[empty_idx], Y[empty_idx]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = remove_empty_review(x_train, y_train)\n",
    "x_test, y_test = remove_empty_review(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 더빙.. 진짜 짜증나네요 목소리'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰 인덱싱 (token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어에 대한 idx 부여\n",
    "def convert_token_to_idx(token_ls):\n",
    "    for tokens in token_ls:\n",
    "        yield [token2idx[token] for token in tokens.split(' ')]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = defaultdict(lambda : len(token2idx)) # token과 index를 매칭시켜주는 딕셔너리\n",
    "pad = token2idx['<PAD>']  # pytorch Variable로 변환하기 위해, 문장의 길이를 맞춰주기 위한 padding \n",
    "\n",
    "x_train = list(convert_token_to_idx(x_train))\n",
    "x_test = list(convert_token_to_idx(x_test))\n",
    "\n",
    "idx2token = {val : key for key,val in token2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 인덱싱 결과 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 원본 텍스트로 변환 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아', '더빙..', '진짜', '짜증나네요', '목소리']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx2token[x] for x in x_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Variable로 변환하기 위해서는 모든 data의 길이(length)가 동일하여야 한다.\n",
    "# 영화 리뷰는 길이가 제각각이므로, 길이를 맞춰주는 작업을 수행\n",
    "# 짧은 문장에는 padding(공간을 채우기 위해 사용하는 더미)을 추가하고,\n",
    "# 긴 문장은 짤라서 줄인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Length를 맞추기 위한 padding\n",
    "def add_padding(token_ls, max_len):\n",
    "    for i, tokens in enumerate(token_ls):\n",
    "        n_token = len(tokens)\n",
    "        \n",
    "        # 길이가 짧으면 padding을 추가\n",
    "        if n_token < max_len:\n",
    "            token_ls[i] += [pad] * (max_len - n_token) # 부족한 만큼 padding을 추가함\n",
    "        \n",
    "        # 길이가 길면, max_len에서 짜름\n",
    "        elif n_token > max_len:\n",
    "            token_ls[i] = tokens[:max_len]\n",
    "    return token_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "x_train = add_padding(x_train, max_len)\n",
    "x_test = add_padding(x_test, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 더빙.. 진짜 짜증나네요 목소리 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2token[x] for x in x_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch 모델 학습을 위해 Data의 type을 Variable 로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch Variable로 변환\n",
    "def convert_to_long_variable(w2i_ls):\n",
    "    return Variable(torch.LongTensor(w2i_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_to_long_variable(x_train)\n",
    "x_test = convert_to_long_variable(x_test)\n",
    "\n",
    "y_train = convert_to_long_variable(y_train)\n",
    "y_test = convert_to_long_variable(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Yoon Kim, 2014, Convolutional Neural Networks for Sentence Classification](https://www.aclweb.org/anthology/D14-1181)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://datawarrior.files.wordpress.com/2016/10/cnn.png?w=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-train된 Embedding은 사용하지 않았습니다.\n",
    "\n",
    "모든 embedding은 랜덤으로 초기화된 상태로 학습을 진행하였습니다. (Rand)\n",
    "\n",
    "hyper-paramter는 자의적인 기준에서 선정하였습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_text(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_words, embed_size, pad_index, hid_size, drop_rate, kernel_size_ls, num_filter, n_class):\n",
    "        super(CNN_text, self).__init__()\n",
    "        \n",
    "        self.pad_index = pad_index              # 단어 embedding 과정에서 제외시킬 padding token\n",
    "        self.embed_size = embed_size            # 임베딩 차원의 크기\n",
    "        self.hid_size = hid_size                # 히든 레이어 갯수\n",
    "        self.drop_rate = drop_rate              # 드롭아웃 비율\n",
    "        self.num_filter = num_filter            # 필터의 갯수 \n",
    "        self.kernel_size_ls = kernel_size_ls    # 각기 다른 필터 사이즈가 담긴 리스트\n",
    "        self.num_kernel = len(kernel_size_ls)   # 필터 사이즈의 종류 수\n",
    "        self.n_class = n_class                  # 카테고리 갯수\n",
    "        \n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=n_words, \n",
    "            embedding_dim=embed_size,\n",
    "            padding_idx=self.pad_index\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # kernel size는 (n-gram, embed_size)이다.\n",
    "        # 커널의 열(column)의 크기는 embed_size와 일치하므로, 단어 임베딩 벡터를 모두 커버한다.\n",
    "        # 따라서, n의 row 크기를 갖는 커널은 한번에 n개의 단어를 커버하는 n-gram 커널이라고 볼 수 있다.\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filter, (kernel_size, embed_size)) for kernel_size in kernel_size_ls])\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(self.num_kernel*num_filter, hid_size), nn.ReLU(), nn.Dropout(drop_rate),\n",
    "            nn.Linear(hid_size, n_class),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed = self.embed(x) # batch_size x max_length x embed_size\n",
    "        embed.unsqueeze_(1)       # batch_size, 1, max_length, embed_size : convolution을 위해 4D로 차원을 조절\n",
    "        \n",
    "        # convolution\n",
    "        conved = [conv(embed).squeeze(3) for conv in self.convs] # [batch_size, num_filter, max_length -kernel_size +1]\n",
    "        \n",
    "        # max_pooling\n",
    "        pooled = [F.max_pool1d(conv, (conv.size(2))).squeeze(2) for conv in conved] # [batch_size, num_kernel, num_filter]\n",
    "            \n",
    "        # dropout\n",
    "        dropouted = [F.dropout(pool, self.drop_rate) for pool in pooled]\n",
    "        \n",
    "        # concatenate\n",
    "        concated = torch.cat(pooled, dim = 1) # [batch_size, num_kernel * num_filter]\n",
    "        logit = self.lin(concated)\n",
    "        \n",
    "        return logit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_words' : len(token2idx),        # 고유한 단어 토큰의 갯수\n",
    "    'embed_size' : 128,                # 임베딩 차원의 크기\n",
    "    'pad_index' : token2idx['<PAD>'],  # 패딩 토큰\n",
    "    'hid_size' : 128,                  # 히든 레이어 갯수\n",
    "    'drop_rate' : 0.5,                 # 드롭아웃 비율          (원문에서는 0.5를 사용)\n",
    "    'kernel_size_ls' : [2,3,4,5],      # 커널 사이즈 리스트        (원문애서는 3,4,5를 사용)\n",
    "    'num_filter' : 32,                 # 각 사이즈 별 커널 갯수 (원문에서는 100을 사용)\n",
    "    'n_class' : 2,                  # 카테고리 갯수\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_text(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_text(\n",
       "  (embed): Embedding(450542, 128, padding_idx=0)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 32, kernel_size=(2, 128), stride=(1, 1))\n",
       "    (1): Conv2d(1, 32, kernel_size=(3, 128), stride=(1, 1))\n",
       "    (2): Conv2d(1, 32, kernel_size=(4, 128), stride=(1, 1))\n",
       "    (3): Conv2d(1, 32, kernel_size=(5, 128), stride=(1, 1))\n",
       "  )\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch : 1,  loss : 9.290302587890626,  accuracy :0.601\n",
      "=================================================================================================\n",
      "Train epoch : 2,  loss : 9.082165478515625,  accuracy :0.627\n",
      "=================================================================================================\n",
      "Train epoch : 3,  loss : 8.87186875,  accuracy :0.644\n",
      "=================================================================================================\n",
      "Train epoch : 4,  loss : 8.628815087890626,  accuracy :0.659\n",
      "=================================================================================================\n",
      "Train epoch : 5,  loss : 8.359375439453125,  accuracy :0.687\n",
      "=================================================================================================\n",
      "Train epoch : 6,  loss : 8.081645849609375,  accuracy :0.702\n",
      "=================================================================================================\n",
      "Train epoch : 7,  loss : 7.792402197265625,  accuracy :0.720\n",
      "=================================================================================================\n",
      "Train epoch : 8,  loss : 7.46921884765625,  accuracy :0.726\n",
      "=================================================================================================\n",
      "Train epoch : 9,  loss : 7.185280224609375,  accuracy :0.744\n",
      "=================================================================================================\n",
      "Train epoch : 10,  loss : 6.86371171875,  accuracy :0.762\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Test Epoch : 10, Test Loss : 0.600 , Test Accuracy : 0.672\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 11,  loss : 6.556491650390625,  accuracy :0.773\n",
      "=================================================================================================\n",
      "Train epoch : 12,  loss : 6.257405517578125,  accuracy :0.785\n",
      "=================================================================================================\n",
      "Train epoch : 13,  loss : 5.95444248046875,  accuracy :0.805\n",
      "=================================================================================================\n",
      "Train epoch : 14,  loss : 5.668301098632813,  accuracy :0.818\n",
      "=================================================================================================\n",
      "Train epoch : 15,  loss : 5.37371103515625,  accuracy :0.826\n",
      "=================================================================================================\n",
      "Train epoch : 16,  loss : 5.08223701171875,  accuracy :0.846\n",
      "=================================================================================================\n",
      "Train epoch : 17,  loss : 4.793897338867188,  accuracy :0.853\n",
      "=================================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6f31dc282b27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "lr = 0.0003\n",
    "batch_size = 10000\n",
    "\n",
    "train_idx = np.arange(x_train.size(0))\n",
    "test_idx = np.arange(x_test.size(0))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr) # 원문에서는 Adadelta 알고리즘을 사용\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "loss_ls = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # input 데이터 순서 섞기\n",
    "    random.shuffle(train_idx)\n",
    "    x_train = x_train[train_idx]\n",
    "    y_train = y_train[train_idx]\n",
    "    train_loss = 0\n",
    "\n",
    "    for start_idx, end_idx in zip(range(0, x_train.size(0), batch_size),\n",
    "                                  range(batch_size, x_train.size(0)+1, batch_size)):\n",
    "        \n",
    "        x_batch = x_train[start_idx : end_idx]\n",
    "        y_batch = y_train[start_idx : end_idx].long()\n",
    "        \n",
    "        scores = model(x_batch)\n",
    "        predict = F.softmax(scores, dim=1).argmax(dim = 1)\n",
    "        \n",
    "        acc = (predict == y_batch).sum().item() / batch_size\n",
    "        \n",
    "        loss = criterion(scores, y_batch)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Train epoch : %s,  loss : %s,  accuracy :%.3f'%(epoch+1, train_loss / batch_size, acc))\n",
    "    print('=================================================================================================')\n",
    "    \n",
    "    loss_ls.append(train_loss)\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model.eval()\n",
    "        scores = model(x_test)\n",
    "        predict = F.softmax(scores, dim=1).argmax(dim = 1)\n",
    "        \n",
    "        acc = (predict == y_test.long()).sum().item() / y_test.size(0)\n",
    "        loss = criterion(scores, y_test.long())\n",
    "        \n",
    "        print('*************************************************************************************************')\n",
    "        print('*************************************************************************************************')\n",
    "        print('Test Epoch : %s, Test Loss : %.03f , Test Accuracy : %.03f'%(epoch+1, loss.item()/y_test.size(0), acc))\n",
    "        print('*************************************************************************************************')\n",
    "        print('*************************************************************************************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
